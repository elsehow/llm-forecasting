#!/usr/bin/env python3
"""
Top-down approach v6: LLM-generated signals with VOI ranking.

Pipeline:
1. identify_uncertainties(question, n=3) — shared with hybrid
2. LLM generates signals per uncertainty
3. rank_signals_by_voi(signals, target) — NEW in v6, shared with bottom-up/hybrid
4. generate_mece_scenarios(signals, voi_floor=0.1) — shared with bottom-up/hybrid

Key difference from v5: signals are now VOI-ranked like other approaches.
This controls another variable between conditions.

Usage:
    uv run python experiments/scenario-construction/gdp_2040/approach_topdown_v6.py --target gdp_2050
    uv run python experiments/scenario-construction/gdp_2040/approach_topdown_v6.py --target renewable_2050
"""

import argparse
import json
import asyncio
import sys
from pathlib import Path
from datetime import datetime

from dotenv import load_dotenv
from pydantic import BaseModel, Field
import litellm

# Import shared utilities
sys.path.insert(0, str(Path(__file__).parent.parent))
from shared.signals import RESOLVABILITY_REQUIREMENTS, rank_signals_by_voi
from shared.uncertainties import identify_uncertainties
from shared.scenarios import generate_mece_scenarios
from shared.config import get_target, TARGETS

load_dotenv()

# Parse arguments
parser = argparse.ArgumentParser(description="Top-down v6 scenario generation")
parser.add_argument(
    "--target",
    choices=list(TARGETS.keys()),
    default="gdp_2050",
    help="Target question to generate scenarios for",
)
parser.add_argument(
    "--n-uncertainties",
    type=int,
    default=3,
    help="Number of uncertainty axes to identify",
)
parser.add_argument(
    "--voi-floor",
    type=float,
    default=0.1,
    help="VOI floor for scenario generation (default 0.1)",
)
args = parser.parse_args()

# Load config
config = get_target(args.target)
TARGET_QUESTION = config.question.text
CONTEXT = config.context

# Paths
OUTPUT_DIR = Path(__file__).parent / "results" / args.target
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Model
MODEL = "claude-sonnet-4-20250514"


# ============================================================
# Signal generation models
# ============================================================

class GeneratedSignal(BaseModel):
    """A signal generated by LLM."""
    text: str = Field(description="The signal as a resolvable prediction market question")
    reasoning: str = Field(description="Why this signal is cruxy for this uncertainty")


class SignalsResponse(BaseModel):
    """Response with generated signals."""
    signals: list[GeneratedSignal]


SIGNAL_GENERATION_PROMPT = """You are generating cruxy signals for a specific uncertainty axis.

TARGET QUESTION: {question}
UNCERTAINTY AXIS: {uncertainty_name}
DESCRIPTION: {uncertainty_description}

Generate {n} signals that would most inform this uncertainty.

{resolvability}

Focus on signals that:
1. Directly relate to this uncertainty axis
2. Resolve in 2-5 years (trackable timeframe)
3. Would significantly update probability if observed
4. Cover different aspects of this uncertainty

For each signal, explain WHY it is cruxy for this uncertainty axis.
"""


async def generate_signals_for_uncertainty(
    question: str,
    uncertainty_name: str,
    uncertainty_description: str,
    n: int = 10,
) -> list[dict]:
    """Generate LLM signals for a specific uncertainty."""

    response = await litellm.acompletion(
        model=MODEL,
        messages=[{
            "role": "user",
            "content": SIGNAL_GENERATION_PROMPT.format(
                question=question,
                uncertainty_name=uncertainty_name,
                uncertainty_description=uncertainty_description,
                n=n,
                resolvability=RESOLVABILITY_REQUIREMENTS,
            )
        }],
        response_format=SignalsResponse,
    )

    result = SignalsResponse.model_validate_json(response.choices[0].message.content)

    return [
        {
            "text": s.text,
            "reasoning": s.reasoning,
            "source": "llm",
            "uncertainty_source": uncertainty_name,
        }
        for s in result.signals
    ]


async def main():
    print("=" * 60)
    print("TOP-DOWN APPROACH v6: LLM-Generated Signals + VOI Ranking")
    print("=" * 60)
    print(f"\nTarget: {TARGET_QUESTION}")
    print(f"Uncertainty axes: {args.n_uncertainties}")
    print(f"VOI floor: {args.voi_floor}")

    # Step 1: Identify key uncertainties (shared with hybrid)
    print("\n[1/4] Identifying key uncertainties...")
    uncertainties = await identify_uncertainties(
        question=TARGET_QUESTION,
        context=CONTEXT,
        n=args.n_uncertainties,
    )

    for i, u in enumerate(uncertainties):
        print(f"\n  {i+1}. {u.name}")
        print(f"     {u.description}")

    # Step 2: Generate signals per uncertainty (LLM-only, no market search)
    print("\n[2/4] Generating signals per uncertainty...")
    all_signals = []
    uncertainty_signals = {}

    for u in uncertainties:
        print(f"\n  Generating for: {u.name}")

        signals = await generate_signals_for_uncertainty(
            question=TARGET_QUESTION,
            uncertainty_name=u.name,
            uncertainty_description=u.description,
            n=10,
        )

        print(f"    Generated {len(signals)} signals")
        print(f"    Sample: {signals[0]['text'][:60]}...")

        uncertainty_signals[u.name] = signals
        all_signals.extend(signals)

    print(f"\n  Total signals: {len(all_signals)}")

    # Step 3: Rank signals by VOI (NEW in v6)
    print("\n[3/4] Ranking signals by VOI...")
    ranked_signals = await rank_signals_by_voi(
        signals=all_signals,
        target=TARGET_QUESTION,
    )

    # Count by VOI threshold
    voi_counts = {
        ">=0.3": sum(1 for s in ranked_signals if s.get("voi", 0) >= 0.3),
        ">=0.2": sum(1 for s in ranked_signals if s.get("voi", 0) >= 0.2),
        ">=0.1": sum(1 for s in ranked_signals if s.get("voi", 0) >= 0.1),
        "<0.1": sum(1 for s in ranked_signals if s.get("voi", 0) < 0.1),
    }
    print(f"\n  VOI distribution:")
    for k, v in voi_counts.items():
        print(f"    {k}: {v}")

    print(f"\n  Top 3 by VOI:")
    for s in ranked_signals[:3]:
        print(f"    VOI={s.get('voi', 0):.2f} [{s['uncertainty_source'][:15]}] {s['text'][:50]}...")

    # Step 4: Generate scenarios (shared with bottom-up/hybrid)
    print("\n[4/4] Generating MECE scenarios...")
    result = await generate_mece_scenarios(
        signals=ranked_signals,
        question=TARGET_QUESTION,
        context=CONTEXT,
        voi_floor=args.voi_floor,
    )

    # Output
    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)

    print(f"\nMECE Reasoning: {result.mece_reasoning}")

    if result.coverage_gaps:
        print(f"\nCoverage Gaps: {result.coverage_gaps}")
    else:
        print(f"\nCoverage Gaps: None")

    print("\n" + "-" * 40)
    for s in result.scenarios:
        print(f"\n### {s.name}")
        print(f"  Outcome Range: {s.outcome_range}")
        print(f"  {s.description}")
        print(f"\n  Why Exclusive: {s.why_exclusive[:80]}...")
        print(f"\n  Key Drivers: {', '.join(s.key_drivers[:3])}")

    # Save results
    output_file = OUTPUT_DIR / f"topdown_v6_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    # Breakdown by uncertainty
    by_uncertainty = {k: len(v) for k, v in uncertainty_signals.items()}

    results = {
        "target": args.target,
        "approach": "topdown_v6_voi",
        "question": {
            "id": config.question.id,
            "text": config.question.text,
            "unit": config.question.unit.type if config.question.unit else None,
            "base_rate": config.question.base_rate,
            "value_range": config.question.value_range,
        },
        "config": {
            "context": config.context,
            "cruxiness_normalizer": config.cruxiness_normalizer,
            "voi_floor": args.voi_floor,
        },
        "current_date": datetime.now().strftime("%Y-%m-%d"),
        "uncertainties": [
            {
                "name": u.name,
                "description": u.description,
                "search_query": u.search_query,
            }
            for u in uncertainties
        ],
        "signals_per_uncertainty": by_uncertainty,
        "total_signals": len(all_signals),
        "signals_above_floor": sum(1 for s in ranked_signals if s.get("voi", 0) >= args.voi_floor),
        "voi_distribution": voi_counts,
        "signals": [
            {
                "text": s["text"],
                "reasoning": s.get("reasoning", ""),
                "source": s["source"],
                "uncertainty_source": s["uncertainty_source"],
                "voi": s.get("voi"),
                "rho": s.get("rho"),
                "rho_reasoning": s.get("rho_reasoning"),
            }
            for s in ranked_signals
        ],
        "scenarios": [
            {
                "name": s.name,
                "description": s.description,
                "outcome_range": s.outcome_range,
                "key_drivers": s.key_drivers,
                "why_exclusive": s.why_exclusive,
                "mapped_signals": s.mapped_signals,
                "indicator_bundle": s.indicator_bundle,
            }
            for s in result.scenarios
        ],
        "mece_reasoning": result.mece_reasoning,
        "coverage_gaps": result.coverage_gaps,
        "created_at": datetime.now().isoformat(),
    }

    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n\nResults saved to: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
